#### Glossary

- Tokenization:  splitting text into meaningful units
- Normalization: the process of transforming text into a single canonical form that it might not have had before